# 정보 이론 

!!! def ""

    정보량 : 그것으로 인하여 감소하는 불확실성의 양이다. 

- 앞면만 2개 있는 동전을 던지면 앞면만 나오기 때문에 그것은 이미 너무 확실해서 어떤 불확실성도 줄여주지 않는다. 따라서 정보가 없다.

    그런데 이 상황을 확률적으로 표현하면 앞면이 나올 확률 100% 와 뒷면이 나올 확률 0% 를 갖는 동전이 주는 정보량은 0 이다 라고 말할 수 있다. 동전을 던지는 사건이 주는 결과가 너무 확실하다는 것은 앞면이 나올 확률이 100% 라는 것이다. 
    
    이를 통하여 정보량은 표본공간의 근원사건이 갖는 확률과 관계있다는 것을 알 수 있다.

- 완벽한 확실성이 존재하는 곳에는 불확실성이 해소될 여지가 없으므로 정보가 존재하지 않는다.

    가령 법정 맹세 선언이나 결혼식 부부 선서에서 당사자들이 "예" 라고 대답할 것이 거의 확실하므로 거기에서 얻을 수 있는 새로운 정보는 거의 없다. 

    그렇다면 당신은 "아니오" 라고 말한 극소수의 사례를 찾고 싶은 충동이 들 것이다. 그러나 정보량의 많고 적음이란 "어떤 특별한 선택" 이 아니라 "어떤 선택으로 인하여 무언가 새로운 것을 알게 될 가능성" 이다. 
    
    가령 앞면이 지나지체 많이 나오는 동전에서도 극히 드물게 뒷면이 나올 수도 있지만, 그런 동전은 평균적으로 예측할 수 있으므로, 정보량이 매우 낮다고 할 수 있다.

- 정보량의 정의에 의하여 가장 많은 양의 불확실성을 해소시켜줄 수 있는 메시지가 가장 많은 정보를 갖고 있다. 

    - 사고 실험 
    
        그렇다면 앞면이 나올 확률이 90% 이고 뒷면이 나올 확률이 10% 인 동전을 생각하자. 이 동전이 주는 정보는 앞면이 나올 확률이 100% 인 동전보다 좀 더 불확실해졌다.

        그러면 앞면이 나올 확률이 80% 이고 뒷면이 나올 확률이 20% 인 동전을 생각하자. 이 동전이 주는 정보는 앞면이 나올 확률이 90% 인 동전보다 좀 더 불확실해졌다.

        이제 앞면이 나올 확률이 60% 이고 뒷면이 나올 확률이 40% 인 동전을 생각하자. 이 동전이 주는 정보는 앞면이 나올 확률이 80% 인 동전보다 더욱 불확실해졌다.

        마지막으로 앞면이 나올 확률이 50% 이고 뒷면이 나올 확률이 50% 인 동전을 생각하자. 이 동전이 주는 정보는 앞면이 나올 확률이 60% 인 동전보다 더욱 불확실해졌다. 왜냐하면 앞면이 나올 확률이 60% 이면 앞면이 나올 것이 좀 더 확실하기 때문이다. 

        그렇다면 앞면이 나올 확률이 40% 이고 뒷면이 나올 확률이 60% 인 동전을 생각하자. 이 동전이 주는 정보는 앞면이 나올 확률이 50% 인 동전보다 확실해졌다. 왜냐하면 뒷면이 나올 것이 더 확실하기 때문이다. 

    이 사고 실험을 통하여 표본공간이 지니고 있는 근원사건의 확률이 모두 균등할때 그 사건이 주는 정보가 가장 불확실하고, 이에따라 정보량이 가장 크다는 것을 알 수 있다. 왜냐하면 어느 한 근원사건의 확률이 다른 근원사건보다 1% 라도 커진다면, 그 사건이 일어날 것이 조금 더 확실해지기 때문이다. 

!!! def ""

    1비트 : 확률적으로 동등한 두 가지 선택지 중 하나를 선택한 데서 비롯된 정보량이다. 

!!! def ""

    균등 확률 표본공간의 정보량 : $n \in \mathbb{N}, i \in \{1, 2, \dots, n\}$ 에 대하여 $n$ 개의 근원사건 $s_i$ 을 갖는 표본공간 $S = \{s_1, s_2, \dots, s_n\}$ 에서 모두 동등한 확률 $p\ (=1/n)$ 로 발생한 사건 $s_i$ 를 $0$ 과 $1$ 로 식별하기 위해서는 비트가 최소한 
    
    $$\log_{2} n = \log_{2} \dfrac{1}{p} = - \log_{2} p$$ 
    
    개 필요한데, 이 비트가 이 사건이 제공하는 정보량(감소되는 불확실성)이다. 

- $0, 1$ 로 구성된 비트의 길이가 $1$ 이면 표현가능한 부호가 $0, 1$ 이 되어 $2$ 가지 근원사건을 식별할 수 있다. 비트의 길이가 $2$ 이면 표현가능한 부호가 $00, 01, 10, 11$ 이 되어 $4$ 가지 근원사건을 식별할 수 있다. 비트의 길이가 $3$ 이면 표현가능한 부호가 $000, 001, 010, 100, 011, 101, 110, 111$ 이 되어 $8$ 가지 근원사건을 식별할 수 있다. 

    그러므로 비트의 길이가 $n$ 이면 $n$ 개의 자리에 $0$ 또는 $1$ 을 대입할 수 있으므로 대입가능한 모든 경우의 수는 $2 \times 2 \times \dots \times 2= 2 ^{n}$, 즉 식별할 수 있는 근원사건이 $2 ^{n}$ 개가 된다.

    그러므로 $a \in \mathbb{N}$ 개의 근원사건을 갖는 표본공간에서 사건들이 동등한 확률 $p(=1/a)$ 로 발생한다고 할때, 이 표본공간의 대상을 식별해야 한다면, 비트의 길이가 $b \in \mathbb{N}$ 가 되어야 하는데, 이들은 $2 ^{b} = a = 1/p$ 의 관계를 갖는다. 따라서 $a$ 개의 근원사건을 갖는 표본공간에서 발생한 사건을 식별하기 위한 최소 비트의 길이 $b$ 는 

    $$ b = \log_{2} a =\log_{2} \dfrac{1}{p}=- \log_{2} p $$

    이다. 또 이 비트의 길이를 정보량(감소되는 불확실성)으로 정의한다.

- 예시

    "맑음", "흐림", "비", "안개" 라는 근원사건을 갖는 표본공간 $S = \{\text{맑음}, \text{흐림}, \text{비 }, \text{안개}\}$ 의 대상을 식별하기 위해서는 각 근원사건에 비트 $00, 01, 10, 11$ 을 부여하면 된다. 즉, 근원사건이 $4$ 개이므로 비트의 길이 $\log_{2} 4 = 2$ 가 필요하다. 

    한편 "맑음" 이라는 근원사건이 $\dfrac{1}{4}$ 의 확률로 발생하므로 $- \log_{2} \dfrac{1}{4} = 2$ 이다. 이렇게 동일한 결과를 얻을 수 있다.

- 예시 

    알파벳은 26자로 되어있는데, 정보통신에서 알파벳 1개를 식별하기 위해서는 $\log_{2} 26 \approx 4.7$ 비트가 필요한다.

- 우리는 $n \in \mathbb{N}$ 개의 근원사건을 갖는 표본공간 $S$ 의 대상 정보를 전달하기 위하여 $\log_{2} n$ 비트가 필요하다는 결론을 얻었다. 그러므로 표본공간 $S$ 의 대상 $s \in \mathbb{N}$ 개의 정보를 전달하기 위해서는 $s \log_{2} n$ 비트가 필요하다.

    - 예시 

        알파벳 $6$ 글자를 전달하기 위해서는 $6 \times \log_{2} 26 \approx 6 \times 4.7 = 28.2$ 비트가 필요하다.

    그러므로 이를 통하여 균등 확률을 갖는 $n \in \mathbb{N}$ 개의 근원사건을 갖는 표본공간 $S$ 에서 발생할 수 있는 $s \in \mathbb{N}$ 개의 근원사건의 정보 $H$ 를

    $$ H = s \log_{2}n = \log_{2} n ^{s} $$

    로 정의할 수 있다.

!!! def ""

    임의 확률 표본공간의 정보량 : $n \in \mathbb{N}, i \in \{1, 2, \dots, n\}$ 에 대하여 $n$ 개의 근원사건 $s_i$ 을 갖는 표본공간 $S = \{s_1, s_2, \dots, s_n\}$ 에서 확률 $p_i = \dfrac{1}{m}$ 로 발생한 사건 $s_i$ 를 $0$ 과 $1$ 로 식별하기 위해서는 비트가 최소한 
    
    $$\log_{2} m = \log_{2} \dfrac{1}{p} = - \log_{2} p$$ 
    
    가 필요하고, 이것이 사건 $s_i$ 가 갖는 정보량(감소되는 불확실성)이다. 

- "맑음", "흐림", "비", "안개" 라는 근원사건을 갖는 표본공간 $S = \{\text{맑음}, \text{흐림}, \text{비 }, \text{안개}\}$ 이 다음과 같은 발생 확률을 갖는다고 하자.

    |날씨|확률|
    |:---:|:---:|
    |맑음|$\dfrac{1}{4}$
    |흐림|$\dfrac{1}{4}$
    |비|$\dfrac{1}{4}$
    |안개|$\dfrac{1}{4}$

    이것을 $0, 1$ 로 식별한다고 하면, 가령 $0$ 으로 "맑음", "흐림" 구별하고, $1$ 로 "비", "안개" 를 구별한 다음, $00, 01$ 으로 "맑음" 과 "흐림"을 구별하고, $10, 11$ 로 "비" 와 "안개" 를 구별하면 된다.

    즉, 각 근원사건에 비트 $00, 01, 10, 11$ 을 부여하면 된다. 이로써 이 표본공간의 사건을 식별하기 위하여 $2$ 비트가 필요하다. 

- 반면 

    |날씨|확률|
    |:---:|:---:|
    |맑음|$\dfrac{1}{2}$
    |흐림|$\dfrac{1}{4}$
    |비|$\dfrac{1}{8}$
    |안개|$\dfrac{1}{8}$

    의 확률을 갖는 표본공간의 근원사건을 $0, 1$ 로 식별해야 한다면, 역시 동일하게 각 근원사건에 비트 $00, 01, 10, 11$ 을 부여하면 된다. 그런데 이는 비효율적이다. 왜냐하면 자주 발생하는 "맑음"($1/2$) 이라는 사건에도 $2$ 비트를 부여했고, 거의 발생하지 않는 사건 "안개"($1/8$) 에도 $2$ 비트를 부여했기 때문이다. 
    
    자주 발생하는 사건에 작은 비트를 부여하는 대신, 거의 발생하지 않는 사건에 큰 비트를 부여하면 모든 사건을 유일하게 특정할 수 있을 뿐만 아니라 비트 길이를 절약할 수 있지 않을까? 

    일단 간단하게 생각해보면, "맑음" 의 확률은 $1/2$ 이고 "흐림", "비", "안개" 의 확률의 합은 $1/2 = 1/4+1/8+1/8$ 이다. 그렇다면 $0$ 으로 "맑음" 을 구별하고 $1$ 로 "흐림", "비", "안개" 를 구별할 수 있다. 

    또한 "흐림" 의 확률이 $1/4$ 이고 "비", "안개" 의 확률이 $1/4 = 1/8+1/8$ 이므로 "흐림" 과 "비", "안개" 의 발생 확률이 동일하다. 그러므로 $10$ 으로 "흐림" 을 구별하고, $11$ 로 "비", "안개" 를 구별할 수 있다. 

    마지막으로 "비" 와 "안개" 의 확률이 동등하므로 $110$ 으로 "비" 를 구별하고, $111$ 로 "안개" 를 구별할 수 있다. 

    정리하자면, "맑음" 에 $0$ 을 부여하고, "흐림" 에 $10$ 을 부여하고, "비" 에 $110$ 을 부여하고 "안개" 에 $111$ 을 부여하면 비트를 최대한 절약할 수 있다. 이렇게 하면 각 근원사건에 부여된 평균 비트량이 $\dfrac{1 + 2 + 3 + 3}{4} = \dfrac{6}{4} = \dfrac{3}{2} = 1.5$ 이다. 

    반면 각 근원사건에 비트 $00, 01, 10, 11$ 을 부여했었다면, 각 근원사건에 부여된 평균 비트량이 $2$ 이다. 이로써 $0.5$ 비트를 절약하게 되었다. 

- 또한 이로써 근원사건이 갖는 확률이 높으면 비트 수를 낮춰야 하고, 확률이 낮으면 비트 수를 높여야 하므로 확률과 비트 수는 반비례 관계를 갖는다는 것을 알 수 있다. 

- 임의 확률 $1/m = p$ 를 $m$ 개의 근원사건과 균등확률을 갖는 표본공간 속 근원사건이라고 본다면 정보량 계산이 편해진다. 왜냐하면 임의의 확률 $p = \dfrac{1}{m}$ 을 갖는 근원사건의 정보량을 $-\log_{2} p$ 로 계산할 수 있기 때문이다. 그러나 이를 위해서는 균등 확률을 갖는 표본공간에서의 어떤 근원사건의 확률이 임의 확률을 갖는 표본공간의 근원사건의 확률과 같다면, 두 사건의 정보량이 같다는 가정이 필요하다. 

    만약 이 가정이 참이라면, 임의 확률 $1/m = p$ 를 표현하는 최단 비트는 $- \log_{2} p$ 라는 결론을 도출해낼 수 있다.

- 정보는 추상적 개념이다. 따라서 정보란 셀 수 없다. 그런데 1948년 섀넌은 정보라는 추상적 자연대상을 수학적 대상물로 사상시켰다. 

    > 혹시 임의의 자연대상을 수학적 대상물로 사상시켰다면 그 대상을 튜링기계로 계산 할 수 있는가? 즉, 아무리 추상적인 자연 대상이라고 할지라도(가령 사랑, 도덕) $PM$ 속으로 사상시키기만 한다면 컴퓨터에서 연산할 수 있을까? 이것이 가능한 것이라면, 어떠한 추상적 자연 대상을 셀 수 있게 만드는 것은 그 대상에 대한 통찰로 그 대상의 최소 구성 요소를 파악하고 그것들을 수학적 구성요소 대응시키는 것이 아닐까?

    섀넌은 어떻게 정보라는 추상 대상물을 $PM$ 속으로 사상시켰는가?

- 위 예시에서 알 수 있듯이 메시지에 담긴 정보의 특성에 따라 정보의 표현에 많은 비트수가 필요할 수도 있고, 적은 비트수가 필요할 수도 있다.

    적은 비트수가 필요하다는 것은 그만큼 정보량이 적다는 것으로 해석할 수 있다. 그러므로 대구의 날씨가 서울의 날씨보다 적은 정보량을 가지고 있다고 볼 수 있다.

    그런데 서울과 대구의 날씨가 다른 것은 확률 뿐이다. 그러므로 정보량과 확률에 모종의 사상 관계가 존재한다는 추론을 합리적으로 할 수 있다. 

    > 사상 관계 존재 추론 방식의 좋은 예시이다.

!!! def ""

    엔트로피 : 이산공간인 표본공간 $S$ 의 확률변수 $X = x$ 가 정보량 $x = - \log_{2} p$ 를 뜻할때

    $$ H(X) = - \sum_{i=1}^{n}p_i \log_{2} p_i $$

    를 엔트로피라고 정의한다. 


- 확률변수란 표본공간의 원소(근원사건)에 실수 $x$ 를 대응시킨 변수 $X=x$ 이다. 우리는 지금까지 셀 수 있는 근원사건을 다루었으므로 표본공간의 근원사건들을 이산확률변수로 나타낼 수 있다. 그런데 이산확률변수의 기댓값, 즉 평균은 

    $$ \mu = E(X) = \sum_{i=1}^{n}x_iP(X=i) = \sum_{i=1}^{n}x_ip_i = x_1p_1 +x_2p_2 + \dots +x_np_n $$

    이다. 그렇다면 표본공간 $S$ 가 제공하는 정보량의 평균은 $x_i = -\log_{2} p_i$ 이므로

    $$ - \sum_{i=1}^{n}p_i \log_{2} p_i $$
 
    이다. 이 양을 엔트로피라고 부르는데, 정보량이란 곧 불확실성을 감소시켜주는 정도를 뜻하기 때문이다. 그러므로 엔트로피가 높으면 어떤 사건의 불확실성이 높다(얻게 되는 정보가 많음)고 볼 수 있고, 엔트로피가 낮으면 어떤 사건이 매우 확실하다(얻는 정보가 거의 없음)고 볼 수 있다.

---

ref:
        
:    http://www.talkorigins.org/faqs/information/shannon.html
 
:    https://www.google.com/search?q=graph+of+the+shannon+information+theory&newwindow=1&client=firefox-b-d&sxsrf=ALeKk00G-WiMqkGNCSOREMgSJuXoDWF1Uw:1603251812466&source=lnms&tbm=isch&sa=X&ved=2ahUKEwiTwpG24sTsAhWYd94KHZDUC_wQ_AUoAXoECBYQAw#imgrc=eletzN9KCFUBhM
 
:    https://www.britannica.com/science/information-theory/Classical-information-theory
 
:    https://hyunw.kim/blog/2017/10/14/Entropy.html